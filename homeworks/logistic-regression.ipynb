{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework #3.1: Logistic Regression\n",
    "\n",
    "In this homework you will learn the concepts of logistic regression by implementing it.\n",
    "\n",
    "Implement the body of each function and test whether you have done right for each of them or not by running the tests. Each function has a test code block just below its definition.\n",
    "\n",
    "- Remember: **m** = number of data items (size of the data set) and **n** = number of features\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import what we need\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid\n",
    "\n",
    "Sigmoid is a non linear function defined as:\n",
    "\n",
    "$$ g(z) = \\frac{1}{1 + e^{-z}} $$\n",
    "\n",
    "![sigmoid](../images/logistic-regression/sigmoid.png)\n",
    "\n",
    "and its derivation is:\n",
    "\n",
    "$$ g'(z) = g(z)(1 - g(z)) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Sigmoid function\n",
    "    \n",
    "    z: an arbitrary matrix (a x b)\n",
    "    \n",
    "    Return: a matrix (a x b) containing sigmoid of each element of z (element wise sigmoid values)\n",
    "    \"\"\"\n",
    "    # YOUR CODE GOES HERE (~ 1 line of code)\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.lines.Line2D at 0x7fa928b97b00>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEACAYAAABI5zaHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n6gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa928b97ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test the sigmoid function by visualization\n",
    "\n",
    "x = np.arange(-10, 10, .1)\n",
    "y = sigmoid(x)\n",
    "plt.plot(x, y)\n",
    "plt.axvline(0, c='gray', linewidth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Function\n",
    "\n",
    "For a single data item $ x_{1 \\times n} $, the hypothesis is the sigmoid of the linear product of $ w_{n \\times 1} $ and $ x_{1 \\times n} $ in addition to bias $ b_{1 \\times 1} $. The result is a number:\n",
    "\n",
    "$$ h_{w,b}(x) = g(w^Tx + b) = \\frac{1}{1 + e^{-w^Tx + b}} $$\n",
    "\n",
    "For a data set $ X_{m \\times n} $ which contains multiple data items stacking vertically, the result is a vector $ h_{m \\times 1} $ which contains predections for all data items:\n",
    "\n",
    "$$ h_{w,b}(X) = g(Xw + b) = \\frac{1}{1 + e^{-Xw + b}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h(X, w, b):\n",
    "    \"\"\"\n",
    "    The hypothesis function\n",
    "    \n",
    "    X: the data set matrix (m x n)\n",
    "    w: the weights vector (n x 1)\n",
    "    b: the bias (1 x 1)\n",
    "    \n",
    "    Return: a vector stacking predictions for all data items (m x 1)\n",
    "    \"\"\"\n",
    "    # YOUR CODE GOES HERE (~ 1 line of code)\n",
    "    return 1/(1+np.exp(-((X.dot(w))+b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hypothesis ok.\n"
     ]
    }
   ],
   "source": [
    "# test the hypothesis\n",
    "\n",
    "X, y, w, b = (np.array([[.1, .2, .3], [.4, .5, .6]]), np.array([[1], [0]]), np.array([[.3], [.4], [.5]]), .5)\n",
    "\n",
    "hyp = h(X, w, b)\n",
    "true = np.array([[0.68135373], [0.75398872]])\n",
    "\n",
    "assert hyp.shape == (X.shape[0], 1), \\\n",
    "       'The result should be in shape ({}, 1). Currently is {}'.format(X.shape[0], hyp.shape)\n",
    "\n",
    "if np.allclose(hyp, true):\n",
    "    print('Hypothesis ok.')\n",
    "else:\n",
    "    print('Hypothesis does not work properly.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function\n",
    "\n",
    "Cost function for a logistic regression model is [Cross Entropy](https://en.wikipedia.org/wiki/Cross_entropy) over data set:\n",
    "\n",
    "$$ \\begin{equation}\n",
    "   \\begin{split}\n",
    "   J_{w,b}(X) &= -\\frac{1}{m}\\sum_{i=1}^m y^{(i)} \\log(h_{w,b}(x^{(i)})) + (1 - y^{(i)}) \\log(1 - h_{w,b}(x^{(i)})) \\\\\n",
    "   &= -\\frac{1}{m}\\sum_{i=1}^m y^{(i)} \\log(\\hat{y}^{(i)}) + (1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)})\n",
    "   \\end{split}\n",
    "   \\end{equation} $$\n",
    "\n",
    "The goal of logistic regression is to minimize this cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Cross Entropy function\n",
    "    \n",
    "    y_true: the vector of true labels of data items (m x 1)\n",
    "    y_pred: the vector of predictions of data items (m x 1)\n",
    "    \n",
    "    Return: a single number representing the cost\n",
    "    \"\"\"\n",
    "    # YOUR CODE GOES HERE (~ 2 lines of code)\n",
    "    return (-1/len(y_true))*np.sum(y_true*np.log(y_pred)+(1-y_true)*np.log(1-y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost function ok.\n"
     ]
    }
   ],
   "source": [
    "# test cost function\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "X, y, w, b = (np.array([[.1, .2, .3], [.4, .5, .6]]), np.array([[1], [0]]), np.array([[.3], [.4], [.5]]), .5)\n",
    "\n",
    "cre = log_loss(y, h(X, w, b))\n",
    "cst = cost(y, h(X, w, b))\n",
    "\n",
    "if np.isclose(cre, cst):\n",
    "    print('Cost function ok.')\n",
    "else:\n",
    "    print('Cost function does not work properly.')\n",
    "    print('Should\\'ve returned:', cre)\n",
    "    print('Returned:', cst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "Gradient descent algorithm tries to find the minimum of a function by starting somewhere on the function and taking small steps through the gradient of the function.\n",
    "\n",
    "In logistic regression, the function we are trying to minimize is the cost function $ J_{w,b}(X) $. The derivations are:\n",
    "\n",
    "$$ \\begin{equation}\n",
    "   \\begin{split}\n",
    "       \\frac{\\partial J_{w,b}(X)}{\\partial w_j}\n",
    "       &= \\frac{1}{m}\\sum_{i=1}^m (h_{w,b}(x^{(i)}) - y^{(i)})x_j^{(i)} \\\\\n",
    "       &= \\frac{1}{m}\\sum_{i=1}^m (\\hat{y}^{(i)} - y^{(i)})x_j^{(i)}\n",
    "   \\end{split}\n",
    "   \\end{equation} $$\n",
    "   \n",
    "$$ \\begin{equation}\n",
    "   \\begin{split}\n",
    "       \\frac{\\partial J_{w,b}(X)}{\\partial b}\n",
    "       &= \\frac{1}{m}\\sum_{i=1}^m (h_{w,b}(x^{(i)}) - y^{(i)}) \\\\\n",
    "       &= \\frac{1}{m}\\sum_{i=1}^m (\\hat{y}^{(i)} - y^{(i)})\n",
    "   \\end{split}\n",
    "   \\end{equation} $$\n",
    "   \n",
    "- Actually these two derivations are the same except that in the second one, $ x_{0}^{(i)} = 1 $.\n",
    "- This cost is similar to the cost of linear regression except that here, there is a sigmoid function over the hypothesis; i.e. $ h_{w,b}(x) = g(w^Tx + b) $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(X, y_true, y_pred):\n",
    "    \"\"\"\n",
    "    The gradient of cost function\n",
    "    \n",
    "    X: the data set matrix (m x n)\n",
    "    y_true: the vector of true labels of data items (m x 1)\n",
    "    y_pred: the vector of predictions of data items (m x 1)\n",
    "    \n",
    "    Return: vector dJ/dw (n x 1) and number dJ/db (1 x 1)\n",
    "    \"\"\"\n",
    "    # YOUR CODE GOES HERE (~ 4 lines of code)\n",
    "    return ((X.T).dot(y_pred-y_true))/len(y_true) , (np.sum(y_pred-y_true))/len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient function ok.\n"
     ]
    }
   ],
   "source": [
    "X, y, w, b = (np.array([[.1, .2, .3], [.4, .5, .6]]), np.array([[1], [0]]), np.array([[.3], [.4], [.5]]), .5)\n",
    "\n",
    "true = (np.array([[0.13486543], [0.15663255], [0.17839968]]), 0.2176712251189869)\n",
    "res = gradient(X, y, h(X, w, b))\n",
    "\n",
    "if np.allclose(res[0], true[0]) and np.isclose(res[1], true[1]):\n",
    "    print('Gradient function ok.')\n",
    "else:\n",
    "    print('Gradient function is not working properly.')\n",
    "    print('should output:', true)\n",
    "    print('Outputted:', res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(X, y_true, y_pred, w, b, alpha):\n",
    "    \"\"\"\n",
    "    This function updates parameters w and b according to their derivations.\n",
    "    It should compute the cost function derivations with respect to w and b first,\n",
    "        then take a step for each parameters in w and b.\n",
    "    \n",
    "    X: the data set matrix (m x n)\n",
    "    y_true: the vector of true labels of data items (m x 1)\n",
    "    y_pred: the vector of predictions of data items (m x 1)\n",
    "    w: the weights vector (n x 1)\n",
    "    b: the bias (1 x 1)\n",
    "    alpha: the learning rate\n",
    "    \n",
    "    Returns: the updated parameters w and b\n",
    "    \"\"\"\n",
    "    # YOUR CODE GOES HERE (~ 4 lines of code)\n",
    "    \n",
    "    N = len(y_pred)\n",
    "\n",
    "    w = w - alpha *  (((X.T).dot(y_pred-y_true))/N) \n",
    "    b = b - alpha * ((np.sum(y_pred-y_true))/N)\n",
    "    return w, b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update parameters function ok.\n"
     ]
    }
   ],
   "source": [
    "# test update_parameters function\n",
    "\n",
    "X, y, w, b = (np.array([[.1, .2, .3], [.4, .5, .6]]), np.array([[1], [0]]), np.array([[.3], [.4], [.5]]), .5)\n",
    "\n",
    "res = update_parameters(X, y, h(X, w, b), w, b, 0.01)\n",
    "true = (np.array([[0.29865135], [0.39843367], [0.498216]]), 0.49782328774881013)\n",
    "\n",
    "if np.allclose(res[0], true[0]) and np.isclose(res[1], true[1]):\n",
    "    print('Update parameters function ok.')\n",
    "else:\n",
    "    print('Update parameters function is not working properly.')\n",
    "    print('should output:', true)\n",
    "    print('Outputted:', res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, alpha, n_iterations):\n",
    "    \"\"\"\n",
    "    The gradient descent algorithm:\n",
    "        1. initialize parameters w and b to zeros (not random)\n",
    "        for i in n_iterations:\n",
    "            2. compute the hypothesis h(X, w, b)\n",
    "            3. update the parameters using function update_parameters(X, y_true, y_pred, w, b, alpha)\n",
    "            4. compute the cost and see the cost is decreasing in each step (optional)\n",
    "            \n",
    "    X: the data set matrix (m x n)\n",
    "    y: the vector of true labels of data items (m x 1)\n",
    "    alpha: the learning rate\n",
    "    n_iterations: number of steps gradient descent should take to converge\n",
    "    \n",
    "    Returns: the best parameters w and b gradient descent found at last\n",
    "    \"\"\"\n",
    "    # YOUR CODE GOES HERE (~ 7 lines of code)\n",
    "    \n",
    "    w= np.zeros( (X.shape[1], 1) ) \n",
    "    b = 0\n",
    "    N = len(y)\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        y_pred = h(X, w, b)\n",
    "        w = w - alpha *  (((X.T).dot(y_pred-y))/N) \n",
    "        b = b - alpha * ((np.sum(y_pred-y))/N)\n",
    "\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient descent function ok.\n"
     ]
    }
   ],
   "source": [
    "# test gradient_descent function\n",
    "\n",
    "true = (np.array([[-0.01488461], [-0.014848], [-0.0148114]]), 0.00036601406503539797)\n",
    "res = gradient_descent(X, y, 0.01, 20)\n",
    "\n",
    "if np.allclose(res[0], true[0]) and np.isclose(res[1], true[1]):\n",
    "    print('Gradient descent function ok.')\n",
    "else:\n",
    "    print('Gradient descent function is not working properly.')\n",
    "    print('should output:', true)\n",
    "    print('Outputted:', res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on Real Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the data\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import scale\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X = X[:, 0:2]\n",
    "X = X[y != 2]\n",
    "X = scale(X)\n",
    "y = y[y != 2]\n",
    "y = y.reshape((y.shape[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train a linear regression model from sklearn\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression().fit(X, y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train our linear regression model\n",
    "\n",
    "w, b = gradient_descent(X, y, 0.1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot the result\n",
    "\n",
    "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, .02), np.arange(y_min, y_max, .02))\n",
    "Z1 = model.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "Z2 = h(np.c_[xx.ravel(), yy.ravel()], w, b)\n",
    "\n",
    "f, axes = plt.subplots(1, 2)\n",
    "\n",
    "axes[0].contourf(xx, yy, Z1.reshape(xx.shape), cmap=plt.cm.binary, alpha=.8)\n",
    "axes[1].contourf(xx, yy, Z2.reshape(xx.shape), cmap=plt.cm.binary, alpha=.8)\n",
    "axes[0].scatter(X[:, 0], X[:, 1], c=model.predict(X), s=10, cmap=plt.cm.winter)\n",
    "axes[1].scatter(X[:, 0], X[:, 1], c=h(X, w, b) > .5, s=10, cmap=plt.cm.winter)\n",
    "axes[0].set_title('Best');\n",
    "axes[1].set_title('Ours');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our hypothesis should be close to the best hypothesis. If they are not the same, raise `n_iterations` argument for our `gradient_descent` function and be careful of `alpha` the learning rate.\n",
    "\n",
    "How well your logistic regression model is? How much happy are you!? :D"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
