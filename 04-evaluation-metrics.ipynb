{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "How to measure a model? How to find out that the model is doing well or just predicting useless?\n",
    "\n",
    "This job is done by **metrics**. There are a bunch of metrics explained in [scikit-learn documentation about model evaluation](http://scikit-learn.org/stable/modules/model_evaluation.html) and we are going to see some of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Test Data\n",
    "\n",
    "<img src=\"images/metrics/train-test.jpg\" width=\"500\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape = (150, 4)\n",
      "y.shape = (150,)\n",
      "\n",
      "X_train.shape = (105, 4)\n",
      "y_train.shape = (105,)\n",
      "X_test.shape = (45, 4)\n",
      "y_test.shape = (45,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "X, y = load_iris(return_X_y=True)\n",
    "print('X.shape =', X.shape)\n",
    "print('y.shape =', y.shape)\n",
    "print()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True)\n",
    "print('X_train.shape =', X_train.shape)\n",
    "print('y_train.shape =', y_train.shape)\n",
    "print('X_test.shape =', X_test.shape)\n",
    "print('y_test.shape =', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Classification Metrics\n",
    "[scikit-learn documentation](http://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "\n",
    "Accuracy is the fraction of the correct predictions:\n",
    "\n",
    "$$ accuracy(y, \\hat{y}) = \\frac{1}{m} \\sum_{i=1}^m 1(y^{(i)} = \\hat{y}^{(i)}) $$\n",
    "\n",
    "where $ 1(x) $ is the [indicator function](https://en.wikipedia.org/wiki/Indicator_function) and $ m $ is the number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = [0, 2, 1, 3]\n",
    "y_true = [0, 1, 2, 3]\n",
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For multiclass classification:\n",
    "\n",
    "<img src=\"images/metrics/confusion-matrix.png\" width=\"500\" />\n",
    "\n",
    "and for binary classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 1],\n",
       "       [2, 3]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_true = [0, 0, 0, 1, 1, 1, 1, 1]\n",
    "y_pred = [0, 1, 0, 1, 0, 1, 0, 1]\n",
    "tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "np.array([[tn, fp],\n",
    "          [fn, tp]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall, Precision & F-Score\n",
    "\n",
    "$$ recall = \\frac{tp}{tp + fp} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ precision = \\frac{tp}{tp + fn} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ F_1 = 2 \\cdot \\frac{precision \\cdot recall}{precision + recall} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[tn fn]\n",
      " [fp, tp]]\n",
      "[[2 0]\n",
      " [1 1]]\n",
      "\n",
      "Recall = 0.5\n",
      "Precision = 1.0\n",
      "F1 = 0.666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "y_pred = [0, 1, 0, 0]\n",
    "y_true = [0, 1, 0, 1]\n",
    "print('[[tn fn]\\n [fp, tp]]')\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "print()\n",
    "print('Recall =', recall_score(y_true, y_pred))\n",
    "print('Precision =', precision_score(y_true, y_pred))\n",
    "print('F1 =', f1_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    class 0       0.67      1.00      0.80         2\n",
      "    class 1       0.00      0.00      0.00         1\n",
      "    class 2       1.00      0.50      0.67         2\n",
      "\n",
      "avg / total       0.67      0.60      0.59         5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_true = [0, 1, 2, 2, 0]\n",
    "y_pred = [0, 0, 2, 1, 0]\n",
    "target_names = ['class 0', 'class 1', 'class 2']\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy\n",
    "\n",
    "$$ l(y, \\hat{y}) = - \\frac{1}{m} \\sum_{i=1}^m y^{(i)} log(\\hat{y}^{(i)}) + (1 - y^{(i)})log(1 - \\hat{y}^{(i)}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17380733669106749"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "y_true = [0, 0, 1, 1]\n",
    "y_pred = [[.9, .1], [.8, .2], [.3, .7], [.01, .99]]\n",
    "log_loss(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Regression Metrics\n",
    "[scikit-learn documentation](http://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Absolute Error\n",
    "\n",
    "$$ MAE(y, \\hat{y}) = \\frac{1}{m} \\sum_{i=1}^m \\mid y^{(i)} - \\hat{y}^{(i)} \\mid $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "y_true = [3, -0.5, 2, 7]\n",
    "y_pred = [2.5, 0.0, 2, 8]\n",
    "mean_absolute_error(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Squared Error\n",
    "\n",
    "$$ MSE(y, \\hat{y}) = \\frac{1}{m} \\sum_{i=1}^m (y^{(i)} - \\hat{y}^{(i)})^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.375"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "y_true = [3, -0.5, 2, 7]\n",
    "y_pred = [2.5, 0.0, 2, 8]\n",
    "mean_squared_error(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $ R^2 $ score\n",
    "\n",
    "From scikit-learn documentation:\n",
    "\n",
    "> The $ R^2 $ score is the [coefficient of determination](https://en.wikipedia.org/wiki/Coefficient_of_determination). It provides a measure of how well future samples are likely to be predicted by the model. Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a $ R^2 $ score of 0.0.\n",
    "\n",
    "$$ R^2(y, \\hat{y}) = 1 - \\frac{\\sum_{i=1}^m (y^{(i)} - \\hat{y}^{(i)})^2}{\\sum_{i=1}^m (y^{(i)} - \\bar{y})^2} $$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
